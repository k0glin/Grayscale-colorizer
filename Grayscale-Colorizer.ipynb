{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc747f8-76d8-4143-9573-28ae9373956f",
   "metadata": {},
   "source": [
    "# Lab 2: Grayscale Colorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8c3ef",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597643a",
   "metadata": {},
   "source": [
    "## 1. Dataset and preprocessing\n",
    "We use the CIFAR-10 dataset (60,000 colour images of size 32×32×3 in 10 classes).  \n",
    "Pixels are normalised to the [0,1] range using `ToTensor()`.  \n",
    "We merge the original train and test splits and randomly divide them into:\n",
    "- 80% training\n",
    "- 10% validation\n",
    "- 10% test\n",
    "\n",
    "This split is done with `random_split` from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec265b88-8cbb-42ac-8ed0-deeb3a32e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(375, 47, 47)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                \n",
    "])\n",
    "\n",
    "all_data = ConcatDataset([\n",
    "    datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform),\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "])\n",
    "\n",
    "n = len(all_data)\n",
    "\n",
    "# Get sizes for train test and val\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = int(0.1 * n)\n",
    "\n",
    "train_set, val_set, test_set = random_split(all_data, [n_train, n_val, n_test])\n",
    "\n",
    "# DataLoaders, why do we shuffle only train?\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6507bb",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Convolutional autoencoder architecture\n",
    "\n",
    "The autoencoder compresses 32×32 RGB images into a low-dimensional latent representation and then reconstructs them.\n",
    "\n",
    "**Encoder:**\n",
    "- Conv2d: 3 → 8 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (32×32 → 16×16)\n",
    "- Conv2d: 8 → 12 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (16×16 → 8×8)\n",
    "- Conv2d: 12 → 16 channels, 3×3 kernel, padding=1, ReLU  \n",
    "\n",
    "The latent space has shape 8×8×16 = 1024 values per image.\n",
    "\n",
    "**Decoder:**\n",
    "- Upsample: factor 2 (8×8 → 16×16)\n",
    "- Conv2d: 16 → 12 channels, 3×3, ReLU\n",
    "- Upsample: factor 2 (16×16 → 32×32)\n",
    "- Conv2d: 12 → 3 channels, 3×3, Sigmoid\n",
    "\n",
    "We train the model to minimise the mean squared error between the input and output images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2cbbdf1-9d2d-4e06-a7b0-3c98dda828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------ MAIN MODEL DEFINITION ------------------------ #\n",
    "class ConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "        #Auto encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "            nn.Conv2d(in_channels = 8, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 16, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 3, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        # tuple (reconstruction, latent)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ SHALLOWER MODEL DEFINITION ------------------------ #\n",
    "class ShallowConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ DEEPER MODEL DEFINITION ------------------------ #\n",
    "class DeepConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 3, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "\n",
    "# ------------------------ BIGGER FILTER MODEL DEFINITION ------------------------ #\n",
    "\n",
    "class BiggerFilterConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(8, 3, 5, stride=1, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e2317",
   "metadata": {},
   "source": [
    "## 3. Training procedure\n",
    "\n",
    "We train with:\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 1e-3\n",
    "- Batch size: 128\n",
    "- Number of epochs: 10\n",
    "\n",
    "At each epoch we compute:\n",
    "- **Training loss** on the training set\n",
    "- **Validation loss** on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# GPU acceleration for faster training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.01, device=device):\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        current_loss = 0.0\n",
    "        for batch_idx, (img, _) in enumerate(train_loader):\n",
    "            \n",
    "            img = img.to(device)\n",
    "            # tuple returned from foward(x)\n",
    "            reconstruction, latent = model(img)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # when the input equal to the target \n",
    "            loss = criterion(reconstruction, img)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item() * img.size(0)\n",
    "\n",
    "        epoch_train_loss = current_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        current_loss_val = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img, _ in val_loader:\n",
    "                img = img.to(device)\n",
    "                reconstruction, latent = model(img)\n",
    "                loss = criterion(reconstruction, img)\n",
    "                current_loss_val += loss.item() * img.size(0)\n",
    "\n",
    "        epoch_val_loss = current_loss_val / len(val_loader.dataset)\n",
    "        validation_losses.append(epoch_val_loss)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {e+1}/{epochs} \"\n",
    "            f\"- Train loss: {epoch_train_loss:.4f}, Validation loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "    return model, train_losses, validation_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "622e04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train loss: 0.0292, Validation loss: 0.009605528756976127943700838330\n",
      "Epoch 2/10 - Train loss: 0.0088, Validation loss: 0.008005475672582786084152672856\n",
      "Epoch 3/10 - Train loss: 0.0077, Validation loss: 0.007111425124108791163968223970\n",
      "Epoch 4/10 - Train loss: 0.0069, Validation loss: 0.006638724034031232186514603910\n",
      "Epoch 5/10 - Train loss: 0.0065, Validation loss: 0.006264751541117827456084388160\n",
      "Epoch 6/10 - Train loss: 0.0062, Validation loss: 0.005997933614999055584871001656\n",
      "Epoch 7/10 - Train loss: 0.0060, Validation loss: 0.005815157110492388294054055820\n",
      "Epoch 8/10 - Train loss: 0.0058, Validation loss: 0.005757048948357502624051384998\n",
      "Epoch 9/10 - Train loss: 0.0056, Validation loss: 0.005529873116562764034931376500\n",
      "Epoch 10/10 - Train loss: 0.0054, Validation loss: 0.005710303083062171630723824478\n"
     ]
    }
   ],
   "source": [
    "baseModel = ConvolutionAutoEncoder()\n",
    "trainedModel, train_losses, validation_losses = train_model(baseModel, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "\n",
    "# Saving the trained model\n",
    "torch.save(trainedModel.state_dict(), 'Base_conv_autoencoder.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4bdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "592fa4e8",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 4.1 Loss curves\n",
    "\n",
    "Figure 1 shows the evolution of the training and validation MSE loss over epochs.  \n",
    "The validation loss decreases and then stabilises after about 10 epochs, which indicates that the autoencoder has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ab57792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(train_losses, validation_losses):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(validation_losses, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Figure 1\")\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a02f",
   "metadata": {},
   "source": [
    "### 4.2 Test performance\n",
    "\n",
    "After training, we evaluate the model on the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device=device):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            batch_size = images.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.005708665053049723388245428879\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_model(trainedModel, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd6f98",
   "metadata": {},
   "source": [
    "### 4.3 Visual inspection\n",
    "\n",
    "Figure 2 illustrates some original input images and their reconstructions.  \n",
    "The reconstructions are slightly blurred but preserve the main objects and colours, which is expected for a simple convolutional autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f6c0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_reconstruction(model, test_loader, device=device, num_images=5, title=\"\"):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Take a single batch from the test (or val) loader\n",
    "    img, _ = next(iter(test_loader)) \n",
    "    img = img.to(device)\n",
    "\n",
    "    # 2. Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        reconstruction, latent = model(img)         \n",
    "\n",
    "    # 3. Move to CPU and (optionally) undo normalization\n",
    "    orig = img.cpu()\n",
    "    rec  = reconstruction.cpu()\n",
    "\n",
    "    # 4. Plot first 7 originals and their reconstructions\n",
    "    n_show = 7\n",
    "    fig, ax = plt.subplots(2, n_show, figsize=(15, 4), dpi=250)\n",
    "\n",
    "    for i in range(n_show):\n",
    "        # [C, H, W] -> [H, W, C]\n",
    "        img_np = orig[i].numpy().transpose((1, 2, 0))\n",
    "        rec_np = rec[i].numpy().transpose((1, 2, 0))\n",
    "\n",
    "        ax[0, i].imshow(img_np)\n",
    "        ax[0, i].axis('off')\n",
    "\n",
    "        ax[1, i].imshow(rec_np)\n",
    "        ax[1, i].axis('off')\n",
    "    ax[0, 0].set_title('Original')\n",
    "    ax[1, 0].set_title('Reconstructed')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # remember to add title to the graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Training model: Base\n",
      "======================\n",
      "[Epoch 1/1] Step 100/375 Batch loss: 0.0214\n",
      "[Epoch 1/1] Step 200/375 Batch loss: 0.0151\n",
      "[Epoch 1/1] Step 300/375 Batch loss: 0.0121\n",
      "Epoch 1/1 - Train: 0.0289, Val: 0.0101\n",
      "\n",
      "======================\n",
      "Training model: Simple\n",
      "======================\n",
      "[Epoch 1/1] Step 100/375 Batch loss: 0.0272\n",
      "[Epoch 1/1] Step 200/375 Batch loss: 0.0153\n",
      "[Epoch 1/1] Step 300/375 Batch loss: 0.0115\n",
      "Epoch 1/1 - Train: 0.0240, Val: 0.0104\n",
      "\n",
      "======================\n",
      "Training model: Deep\n",
      "======================\n",
      "[Epoch 1/1] Step 100/375 Batch loss: 0.0227\n",
      "[Epoch 1/1] Step 200/375 Batch loss: 0.0162\n",
      "[Epoch 1/1] Step 300/375 Batch loss: 0.0147\n",
      "Epoch 1/1 - Train: 0.0205, Val: 0.0138\n",
      "\n",
      "======================\n",
      "Training model: WideKernel\n",
      "======================\n",
      "[Epoch 1/1] Step 100/375 Batch loss: 0.0155\n",
      "[Epoch 1/1] Step 200/375 Batch loss: 0.0121\n",
      "[Epoch 1/1] Step 300/375 Batch loss: 0.0111\n",
      "Epoch 1/1 - Train: 0.0165, Val: 0.0098\n",
      "\n",
      "======================\n",
      "Training model: Strided\n",
      "======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634: UserWarning: Using a target size (torch.Size([128, 3, 32, 32])) that is different to the input size (torch.Size([128, 3, 16, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m======================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m trained_model, train_losses, validation_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Saving the trained model\u001b[39;00m\n\u001b[32m     21\u001b[39m torch.save(trained_model.state_dict(), \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname.lower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_conv_autoencoder.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001b[39m\n\u001b[32m     25\u001b[39m optimizer.zero_grad()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# when the input equal to the target \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m loss.backward()\n\u001b[32m     31\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634\u001b[39m, in \u001b[36mMSELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\functional.py:3864\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3862\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3864\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3866\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3867\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.size() != \u001b[38;5;28minput\u001b[39m.size():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\functional.py:77\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "models_to_test = {\n",
    "    \"Base\": ConvolutionAutoEncoder(),\n",
    "    \"Simple\": ShallowConvolutionAutoEncoder(),\n",
    "    \"Deep\": DeepConvolutionAutoEncoder(),\n",
    "    \"WideKernel\": BiggerFilterConvolutionAutoEncoder()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models_to_test.items():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"Training model: {name}\")\n",
    "    print(f\"======================\")\n",
    "\n",
    "    trained_model, train_losses, validation_losses = train_model(model, train_loader, val_loader, epochs=1, learning_rate=1e-3)\n",
    "    \n",
    "    # Saving the trained model\n",
    "    torch.save(trained_model.state_dict(), f'{name.lower()}_conv_autoencoder.pth')\n",
    "\n",
    "    test_loss = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "    results[name] = {\n",
    "        \"model\": trained_model,\n",
    "        \"val_loss\": validation_losses,\n",
    "        \"test_loss\": test_loss\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Show summary of results\n",
    "print(\"\\n Model Performance Summary:\")\n",
    "for name, entry in results.items():\n",
    "    print(f\"{name:<15} | Test Loss: {entry['test_loss']:.6f}\")\n",
    "\n",
    "# Optionally, show all the reconstructions\n",
    "for name, entry in results.items():\n",
    "    model = entry[\"model\"]\n",
    "    show_reconstruction(model, test_loader, device=device, title=f\"Reconstruction - {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089610fa",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6729ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48000, 6000, 6000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cell: Dataset for colorization (gray -> RGB) ---\n",
    "from torch.utils.data import Dataset, ConcatDataset, random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Base CIFAR-10 without transforms (so we can build paired input/target)\n",
    "base_train = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=None)\n",
    "base_test  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=None)\n",
    "\n",
    "class GrayToColorCIFAR(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x: grayscale image replicated to 3 channels, shape [3, 32, 32], in [0,1]\n",
    "      y: original RGB image, shape [3, 32, 32], in [0,1]\n",
    "    \"\"\"\n",
    "    def __init__(self, baseds):\n",
    "        self.baseds = baseds\n",
    "        self.t_in  = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.t_tgt = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.baseds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.baseds[idx]     # PIL RGB\n",
    "        x = self.t_in(img)            # [3,H,W] grayscale replicated to 3ch\n",
    "        y = self.t_tgt(img)           # [3,H,W] RGB\n",
    "        return x, y\n",
    "\n",
    "# Merge original train+test and re-split 80/10/10\n",
    "all_color = ConcatDataset([GrayToColorCIFAR(base_train), GrayToColorCIFAR(base_test)])\n",
    "n = len(all_color)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "train_set_c, val_set_c, test_set_c = random_split(all_color, [n_train, n_val, n_test])\n",
    "\n",
    "# Reuse your previous batch size if you had one; otherwise 128 is fine\n",
    "batch_size = 128\n",
    "train_loader_c = DataLoader(train_set_c, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "val_loader_c   = DataLoader(val_set_c,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader_c  = DataLoader(test_set_c,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "len(train_set_c), len(val_set_c), len(test_set_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7285e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Model + optimizer for colorization ---\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "model_c = ConvolutionAutoEncoder().to(device)   # reuse your class from earlier\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_c = optim.Adam(model_c.parameters(), lr=0.01)  # 1e-3 is usually smoother than 1e-2\n",
    "epochs_c = 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
