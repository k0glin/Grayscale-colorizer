{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc747f8-76d8-4143-9573-28ae9373956f",
   "metadata": {},
   "source": [
    "# Lab 2: Grayscale Colorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8c3ef",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597643a",
   "metadata": {},
   "source": [
    "## 1. Dataset and preprocessing\n",
    "We use the CIFAR-10 dataset (60,000 colour images of size 32×32×3 in 10 classes).  \n",
    "Pixels are normalised to the [0,1] range using `ToTensor()`.  \n",
    "We merge the original train and test splits and randomly divide them into:\n",
    "- 80% training\n",
    "- 10% validation\n",
    "- 10% test\n",
    "\n",
    "This split is done with `random_split` from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec265b88-8cbb-42ac-8ed0-deeb3a32e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                \n",
    "])\n",
    "\n",
    "all_data = ConcatDataset([\n",
    "    datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform),\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "])\n",
    "\n",
    "n = len(all_data)\n",
    "\n",
    "# Get sizes for train test and val\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = int(0.1 * n)\n",
    "\n",
    "train_set, val_set, test_set = random_split(all_data, [n_train, n_val, n_test])\n",
    "\n",
    "# DataLoaders, why do we shuffle only train?\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_set, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6507bb",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Convolutional autoencoder architecture\n",
    "\n",
    "The autoencoder compresses 32×32 RGB images into a low-dimensional latent representation and then reconstructs them.\n",
    "\n",
    "**Encoder:**\n",
    "- Conv2d: 3 → 8 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (32×32 → 16×16)\n",
    "- Conv2d: 8 → 12 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (16×16 → 8×8)\n",
    "- Conv2d: 12 → 16 channels, 3×3 kernel, padding=1, ReLU  \n",
    "\n",
    "The latent space has shape 8×8×16 = 1024 values per image.\n",
    "\n",
    "**Decoder:**\n",
    "- Upsample: factor 2 (8×8 → 16×16)\n",
    "- Conv2d: 16 → 12 channels, 3×3, ReLU\n",
    "- Upsample: factor 2 (16×16 → 32×32)\n",
    "- Conv2d: 12 → 3 channels, 3×3, Sigmoid\n",
    "\n",
    "We train the model to minimise the mean squared error between the input and output images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2cbbdf1-9d2d-4e06-a7b0-3c98dda828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------ MAIN MODEL DEFINITION ------------------------ #\n",
    "class ConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "        #Auto encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "            nn.Conv2d(in_channels = 8, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 16, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 3, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        # tuple (reconstruction, latent)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ SHALLOWER MODEL DEFINITION ------------------------ #\n",
    "class ShallowConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ DEEPER MODEL DEFINITION ------------------------ #\n",
    "class DeepConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 3, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "\n",
    "# ------------------------ BIGGER FILTER MODEL DEFINITION ------------------------ #\n",
    "\n",
    "class BiggerFilterConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(8, 3, 5, stride=1, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ LARGER STRIDE MODEL DEFINITION ------------------------ #\n",
    "class StridedConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "        #Auto encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "            nn.Conv2d(in_channels = 8, out_channels = 12, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 16, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 12, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 3, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        # tuple (reconstruction, latent)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e2317",
   "metadata": {},
   "source": [
    "## 3. Training procedure\n",
    "\n",
    "We train with:\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 1e-3\n",
    "- Batch size: 128\n",
    "- Number of epochs: 10\n",
    "\n",
    "At each epoch we compute:\n",
    "- **Training loss** on the training set\n",
    "- **Validation loss** on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "528a2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# GPU acceleration for faster training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.01, device=device):\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        current_loss = 0.0\n",
    "        for batch_idx, (img, _) in enumerate(train_loader):\n",
    "            \n",
    "            img = img.to(device)\n",
    "            # tuple returned from foward(x)\n",
    "            reconstruction, latent = model(img)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # when the input equal to the target \n",
    "            loss = criterion(reconstruction, img)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item() * img.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"[Epoch {e+1}/{epochs}] \"\n",
    "                    f\"Step {batch_idx+1}/{len(train_loader)} \"\n",
    "                    f\"Batch loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_train_loss = current_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        current_loss_val = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img, _ in val_loader:\n",
    "                img = img.to(device)\n",
    "                reconstruction, latent = model(img)\n",
    "                loss = criterion(reconstruction, img)\n",
    "                current_loss_val += loss.item() * img.size(0)\n",
    "\n",
    "        epoch_val_loss = current_loss_val / len(val_loader.dataset)\n",
    "        validation_losses.append(epoch_val_loss)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {e+1}/{epochs} \"\n",
    "            f\"- Train: {epoch_train_loss:.4f}, Val: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "    return model, train_losses, validation_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "622e04a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m baseModel = ConvolutionAutoEncoder()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainedModel, train_losses, validation_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Saving the trained model\u001b[39;00m\n\u001b[32m      5\u001b[39m torch.save(trainedModel.state_dict(), \u001b[33m'\u001b[39m\u001b[33mBase_conv_autoencoder.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001b[39m\n\u001b[32m     22\u001b[39m img = img.to(device)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# tuple returned from foward(x)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m reconstruction, latent = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m optimizer.zero_grad()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# when the input equal to the target \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mConvolutionAutoEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     31\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# tuple (reconstruction, latent)\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out, h\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "baseModel = ConvolutionAutoEncoder()\n",
    "trainedModel, train_losses, validation_losses = train_model(baseModel, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "\n",
    "# Saving the trained model\n",
    "torch.save(trainedModel.state_dict(), 'Base_conv_autoencoder.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd8af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0094\n",
      "Test Loss: 0.0094\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, device=device):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            batch_size = images.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "val_loss = evaluate_model(trainedModel, val_loader)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fa4e8",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 4.1 Loss curves\n",
    "\n",
    "Figure 1 shows the evolution of the training and validation MSE loss over epochs.  \n",
    "The validation loss decreases and then stabilises after about 10 epochs, which indicates that the autoencoder has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ab57792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(train_losses, validation_losses):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(validation_losses, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Figure 1\")\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a02f",
   "metadata": {},
   "source": [
    "### 4.2 Test performance\n",
    "\n",
    "After training, we evaluate the model on the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a69e255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0094\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_model(trainedModel, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd6f98",
   "metadata": {},
   "source": [
    "### 4.3 Visual inspection\n",
    "\n",
    "Figure 2 illustrates some original input images and their reconstructions.  \n",
    "The reconstructions are slightly blurred but preserve the main objects and colours, which is expected for a simple convolutional autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f6c0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_reconstruction(model, test_loader, device=device, num_images=5, title=\"\"):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Take a single batch from the test (or val) loader\n",
    "    img, _ = next(iter(test_loader)) \n",
    "    img = img.to(device)\n",
    "\n",
    "    # 2. Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        reconstruction, latent = model(img)         \n",
    "\n",
    "    # 3. Move to CPU and (optionally) undo normalization\n",
    "    orig = img.cpu()\n",
    "    rec  = reconstruction.cpu()\n",
    "\n",
    "    # 4. Plot first 7 originals and their reconstructions\n",
    "    n_show = 7\n",
    "    fig, ax = plt.subplots(2, n_show, figsize=(15, 4), dpi=250)\n",
    "\n",
    "    for i in range(n_show):\n",
    "        # [C, H, W] -> [H, W, C]\n",
    "        img_np = orig[i].numpy().transpose((1, 2, 0))\n",
    "        rec_np = rec[i].numpy().transpose((1, 2, 0))\n",
    "\n",
    "        ax[0, i].imshow(img_np)\n",
    "        ax[0, i].axis('off')\n",
    "\n",
    "        ax[1, i].imshow(rec_np)\n",
    "        ax[1, i].axis('off')\n",
    "    ax[0, 0].set_title('Original')\n",
    "    ax[1, 0].set_title('Reconstructed')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # remember to add title to the graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Training model: Base\n",
      "======================\n",
      "[Epoch 1/5] Step 100/375 Batch loss: 0.0223\n",
      "[Epoch 1/5] Step 200/375 Batch loss: 0.0123\n",
      "[Epoch 1/5] Step 300/375 Batch loss: 0.0100\n",
      "Epoch 1/5 - Train: 0.0324, Val: 0.0098\n",
      "Validation/Test Loss for Base: 0.009768\n",
      "\n",
      "======================\n",
      "Training model: Simple\n",
      "======================\n",
      "[Epoch 1/5] Step 100/375 Batch loss: 0.0221\n",
      "[Epoch 1/5] Step 200/375 Batch loss: 0.0131\n",
      "[Epoch 1/5] Step 300/375 Batch loss: 0.0106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m======================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m trained_model, train_losses, validation_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Evaluate on test or validation set\u001b[39;00m\n\u001b[32m     21\u001b[39m val_loss = validate_model(trained_model, val_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# when the input equal to the target \u001b[39;00m\n\u001b[32m     28\u001b[39m loss = criterion(reconstruction, img)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m optimizer.step()\n\u001b[32m     33\u001b[39m current_loss += loss.item() * img.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "models_to_test = {\n",
    "    \"Base\": ConvolutionAutoEncoder(),\n",
    "    \"Simple\": ShallowConvolutionAutoEncoder(),\n",
    "    \"Deep\": DeepConvolutionAutoEncoder(),\n",
    "    \"WideKernel\": BiggerFilterConvolutionAutoEncoder(),\n",
    "    \"Strided\": StridedConvolutionAutoEncoder()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models_to_test.items():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"Training model: {name}\")\n",
    "    print(f\"======================\")\n",
    "\n",
    "    trained_model, train_losses, validation_losses = train_model(model, train_loader, val_loader, epochs=10, learning_rate=1e-3)\n",
    "    \n",
    "    # Saving the trained model\n",
    "    torch.save(trained_model.state_dict(), f'{name.lower()}_conv_autoencoder.pth')\n",
    "\n",
    "    # Evaluate on test or validation set\n",
    "    val_loss = evaluate_model(trained_model, val_loader)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"model\": trained_model,\n",
    "        \"val_loss\": val_loss\n",
    "    }\n",
    "\n",
    "    print(f\"Validation/Test Loss for {name}: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Show summary of results\n",
    "print(\"\\n Model Performance Summary:\")\n",
    "for name, entry in results.items():\n",
    "    print(f\"{name:<15} | Validation/Test Loss: {entry['val_loss']:.6f}\")\n",
    "\n",
    "# Optionally, show all the reconstructions\n",
    "for name, entry in results.items():\n",
    "    model = entry[\"model\"]\n",
    "    show_reconstruction(model, test_loader, device=device, title=f\"Reconstruction - {name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
