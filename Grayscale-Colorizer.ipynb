{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc747f8-76d8-4143-9573-28ae9373956f",
   "metadata": {},
   "source": [
    "# Lab 2: Grayscale Colorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8c3ef",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597643a",
   "metadata": {},
   "source": [
    "## 1. Dataset and preprocessing\n",
    "We use the CIFAR-10 dataset (60,000 colour images of size 32×32×3 in 10 classes).  \n",
    "Pixels are normalised to the [0,1] range using `ToTensor()`.  \n",
    "We merge the original train and test splits and randomly divide them into:\n",
    "- 80% training\n",
    "- 10% validation\n",
    "- 10% test\n",
    "\n",
    "This split is done with `random_split` from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec265b88-8cbb-42ac-8ed0-deeb3a32e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                \n",
    "])\n",
    "\n",
    "all_data = ConcatDataset([\n",
    "    datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform),\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "])\n",
    "\n",
    "n = len(all_data)\n",
    "\n",
    "# Get sizes for train test and val\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = int(0.1 * n)\n",
    "\n",
    "train_set, val_set, test_set = random_split(all_data, [n_train, n_val, n_test])\n",
    "\n",
    "# DataLoaders, why do we shuffle only train?\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_set, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6507bb",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Convolutional autoencoder architecture\n",
    "\n",
    "The autoencoder compresses 32×32 RGB images into a low-dimensional latent representation and then reconstructs them.\n",
    "\n",
    "**Encoder:**\n",
    "- Conv2d: 3 → 8 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (32×32 → 16×16)\n",
    "- Conv2d: 8 → 12 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (16×16 → 8×8)\n",
    "- Conv2d: 12 → 16 channels, 3×3 kernel, padding=1, ReLU  \n",
    "\n",
    "The latent space has shape 8×8×16 = 1024 values per image.\n",
    "\n",
    "**Decoder:**\n",
    "- Upsample: factor 2 (8×8 → 16×16)\n",
    "- Conv2d: 16 → 12 channels, 3×3, ReLU\n",
    "- Upsample: factor 2 (16×16 → 32×32)\n",
    "- Conv2d: 12 → 3 channels, 3×3, Sigmoid\n",
    "\n",
    "We train the model to minimise the mean squared error between the input and output images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2cbbdf1-9d2d-4e06-a7b0-3c98dda828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------ MAIN MODEL DEFINITION ------------------------ #\n",
    "class ConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "        #Auto encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "            nn.Conv2d(in_channels = 8, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 16, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 3, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        # tuple (reconstruction, latent)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ SHALLOWER MODEL DEFINITION ------------------------ #\n",
    "class ShallowConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ DEEPER MODEL DEFINITION ------------------------ #\n",
    "class DeepConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 3, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "\n",
    "# ------------------------ BIGGER FILTER MODEL DEFINITION ------------------------ #\n",
    "\n",
    "class BiggerFilterConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(8, 3, 5, stride=1, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ LARGER STRIDE MODEL DEFINITION ------------------------ #\n",
    "class StridedConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "        #Auto encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "            nn.Conv2d(in_channels = 8, out_channels = 12, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 16, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 12, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 3, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        # tuple (reconstruction, latent)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e2317",
   "metadata": {},
   "source": [
    "## 3. Training procedure\n",
    "\n",
    "We train with:\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 1e-3\n",
    "- Batch size: 128\n",
    "- Number of epochs: 10\n",
    "\n",
    "At each epoch we compute:\n",
    "- **Training loss** on the training set\n",
    "- **Validation loss** on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# GPU acceleration for faster training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.01, device=device):\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        current_loss = 0.0\n",
    "        for batch_idx, (img, _) in enumerate(train_loader):\n",
    "            \n",
    "            img = img.to(device)\n",
    "            # tuple returned from foward(x)\n",
    "            reconstruction, latent = model(img)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # when the input equal to the target \n",
    "            loss = criterion(reconstruction, img)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item() * img.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"[Epoch {e+1}/{epochs}] \"\n",
    "                    f\"Step {batch_idx+1}/{len(train_loader)} \"\n",
    "                    f\"Batch loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_train_loss = current_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        current_loss_val = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img, _ in val_loader:\n",
    "                img = img.to(device)\n",
    "                reconstruction, latent = model(img)\n",
    "                loss = criterion(reconstruction, img)\n",
    "                current_loss_val += loss.item() * img.size(0)\n",
    "\n",
    "        epoch_val_loss = current_loss_val / len(val_loader.dataset)\n",
    "        validation_losses.append(epoch_val_loss)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {e+1}/{epochs} \"\n",
    "            f\"- Train: {epoch_train_loss:.4f}, Val: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "    return model, train_losses, validation_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "622e04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Step 100/375 Batch loss: 0.0218\n",
      "[Epoch 1/10] Step 200/375 Batch loss: 0.0160\n",
      "[Epoch 1/10] Step 300/375 Batch loss: 0.0110\n",
      "Epoch 1/10 - Train: 0.0335, Val: 0.0101\n"
     ]
    }
   ],
   "source": [
    "baseModel = ConvolutionAutoEncoder()\n",
    "trainedModel, train_losses, validation_losses = train_model(baseModel, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fcd8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device=device):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fa4e8",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 4.1 Loss curves\n",
    "\n",
    "Figure 1 shows the evolution of the training and validation MSE loss over epochs.  \n",
    "The validation loss decreases and then stabilises after about 10 epochs, which indicates that the autoencoder has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ab57792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(train_losses, validation_losses):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(validation_losses, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Figure 1\")\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a02f",
   "metadata": {},
   "source": [
    "### 4.2 Test performance\n",
    "\n",
    "After training, we evaluate the model on the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a07295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device=device):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, _ in test_loader:\n",
    "            img = img.to(device)\n",
    "            reconstruction, latent = model(img) \n",
    "            # MSE between input and reconstruction\n",
    "            loss = criterion(reconstruction, img)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Test MSE: {test_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd6f98",
   "metadata": {},
   "source": [
    "### 4.3 Visual inspection\n",
    "\n",
    "Figure 2 illustrates some original input images and their reconstructions.  \n",
    "The reconstructions are slightly blurred but preserve the main objects and colours, which is expected for a simple convolutional autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f6c0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_reconstruction(model, test_loader, device=device, num_images=5, title=\"\"):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Take a single batch from the test (or val) loader\n",
    "    img, _ = next(iter(test_loader)) \n",
    "    img = img.to(device)\n",
    "\n",
    "    # 2. Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        reconstruction, latent = model(img)         \n",
    "\n",
    "    # 3. Move to CPU and (optionally) undo normalization\n",
    "    orig = img.cpu()\n",
    "    rec  = reconstruction.cpu()\n",
    "\n",
    "    # 4. Plot first 7 originals and their reconstructions\n",
    "    n_show = 7\n",
    "    fig, ax = plt.subplots(2, n_show, figsize=(15, 4), dpi=250)\n",
    "\n",
    "    for i in range(n_show):\n",
    "        # [C, H, W] -> [H, W, C]\n",
    "        img_np = orig[i].numpy().transpose((1, 2, 0))\n",
    "        rec_np = rec[i].numpy().transpose((1, 2, 0))\n",
    "\n",
    "        ax[0, i].imshow(img_np)\n",
    "        ax[0, i].axis('off')\n",
    "\n",
    "        ax[1, i].imshow(rec_np)\n",
    "        ax[1, i].axis('off')\n",
    "    ax[0, 0].set_title('Original')\n",
    "    ax[1, 0].set_title('Reconstructed')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # remember to add title to the graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ee0c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Training model: Base\n",
      "======================\n",
      "[Epoch 1/5] Step 100/375 Batch loss: 0.0223\n",
      "[Epoch 1/5] Step 200/375 Batch loss: 0.0123\n",
      "[Epoch 1/5] Step 300/375 Batch loss: 0.0100\n",
      "Epoch 1/5 - Train: 0.0324, Val: 0.0098\n",
      "Validation/Test Loss for Base: 0.009768\n",
      "\n",
      "======================\n",
      "Training model: Simple\n",
      "======================\n",
      "[Epoch 1/5] Step 100/375 Batch loss: 0.0221\n",
      "[Epoch 1/5] Step 200/375 Batch loss: 0.0131\n",
      "[Epoch 1/5] Step 300/375 Batch loss: 0.0106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m======================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m trained_model, train_losses, validation_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Evaluate on test or validation set\u001b[39;00m\n\u001b[32m     21\u001b[39m val_loss = validate_model(trained_model, val_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# when the input equal to the target \u001b[39;00m\n\u001b[32m     28\u001b[39m loss = criterion(reconstruction, img)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m optimizer.step()\n\u001b[32m     33\u001b[39m current_loss += loss.item() * img.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "models_to_test = {\n",
    "    \"Base\": ConvolutionAutoEncoder(),\n",
    "    \"Simple\": ShallowConvolutionAutoEncoder(),\n",
    "    \"Deep\": DeepConvolutionAutoEncoder(),\n",
    "    \"WideKernel\": BiggerFilterConvolutionAutoEncoder(),\n",
    "    \"Strided\": StridedConvolutionAutoEncoder()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models_to_test.items():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"Training model: {name}\")\n",
    "    print(f\"======================\")\n",
    "\n",
    "    trained_model, train_losses, validation_losses = train_model(model, train_loader, val_loader, epochs=10, learning_rate=1e-3)\n",
    "    \n",
    "    # Evaluate on test or validation set\n",
    "    val_loss = validate_model(trained_model, val_loader)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"model\": trained_model,\n",
    "        \"val_loss\": val_loss\n",
    "    }\n",
    "\n",
    "    print(f\"Validation/Test Loss for {name}: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Show summary of results\n",
    "print(\"\\n Model Performance Summary:\")\n",
    "for name, entry in results.items():\n",
    "    print(f\"{name:<15} | Validation/Test Loss: {entry['val_loss']:.6f}\")\n",
    "\n",
    "# Optionally, show all the reconstructions\n",
    "for name, entry in results.items():\n",
    "    model = entry[\"model\"]\n",
    "    show_reconstruction(model, test_loader, device=device, title=f\"Reconstruction - {name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
