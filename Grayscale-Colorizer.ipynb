{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc747f8-76d8-4143-9573-28ae9373956f",
   "metadata": {},
   "source": [
    "# Lab 2: Grayscale Colorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8c3ef",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597643a",
   "metadata": {},
   "source": [
    "## 1. Dataset and preprocessing\n",
    "We use the CIFAR-10 dataset (60,000 colour images of size 32×32×3 in 10 classes).  \n",
    "Pixels are normalised to the [0,1] range using `ToTensor()`.  \n",
    "We merge the original train and test splits and randomly divide them into:\n",
    "- 80% training\n",
    "- 10% validation\n",
    "- 10% test\n",
    "\n",
    "This split is done with `random_split` from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec265b88-8cbb-42ac-8ed0-deeb3a32e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                \n",
    "])\n",
    "\n",
    "all_data = ConcatDataset([\n",
    "    datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform),\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "])\n",
    "\n",
    "n = len(all_data)\n",
    "\n",
    "# Get sizes for train test and val\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = int(0.1 * n)\n",
    "\n",
    "train_set, val_set, test_set = random_split(all_data, [n_train, n_val, n_test])\n",
    "\n",
    "# DataLoaders, why do we shuffle only train?\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_set, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6507bb",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Convolutional autoencoder architecture\n",
    "\n",
    "The autoencoder compresses 32×32 RGB images into a low-dimensional latent representation and then reconstructs them.\n",
    "\n",
    "**Encoder:**\n",
    "- Conv2d: 3 → 8 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (32×32 → 16×16)\n",
    "- Conv2d: 8 → 12 channels, 3×3 kernel, padding=1, ReLU\n",
    "- MaxPool2d: 2×2 (16×16 → 8×8)\n",
    "- Conv2d: 12 → 16 channels, 3×3 kernel, padding=1, ReLU  \n",
    "\n",
    "The latent space has shape 8×8×16 = 1024 values per image.\n",
    "\n",
    "**Decoder:**\n",
    "- Upsample: factor 2 (8×8 → 16×16)\n",
    "- Conv2d: 16 → 12 channels, 3×3, ReLU\n",
    "- Upsample: factor 2 (16×16 → 32×32)\n",
    "- Conv2d: 12 → 3 channels, 3×3, Sigmoid\n",
    "\n",
    "We train the model to minimise the mean squared error between the input and output images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2cbbdf1-9d2d-4e06-a7b0-3c98dda828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------ MAIN MODEL DEFINITION ------------------------ #\n",
    "class ConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "        #Auto encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "            nn.Conv2d(in_channels = 8, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 16, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 12, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 3, kernel_size = 3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        # tuple (reconstruction, latent)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ SHALLOWER MODEL DEFINITION ------------------------ #\n",
    "class ShallowConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ DEEPER MODEL DEFINITION ------------------------ #\n",
    "class DeepConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 3, 3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "\n",
    "# ------------------------ BIGGER FILTER MODEL DEFINITION ------------------------ #\n",
    "\n",
    "class BiggerFilterConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(16, 8, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(8, 3, 5, stride=1, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        return out, h\n",
    "\n",
    "# ------------------------ LARGER STRIDE MODEL DEFINITION ------------------------ #\n",
    "class StridedConvolutionAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "        #Auto encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
    "            nn.Conv2d(in_channels = 8, out_channels = 12, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 16, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 12, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels = 12, out_channels = 3, kernel_size = 3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        out = self.decoder(h)\n",
    "        # tuple (reconstruction, latent)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e2317",
   "metadata": {},
   "source": [
    "## 3. Training procedure\n",
    "\n",
    "We train with:\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 1e-3\n",
    "- Batch size: 128\n",
    "- Number of epochs: 10\n",
    "\n",
    "At each epoch we compute:\n",
    "- **Training loss** on the training set\n",
    "- **Validation loss** on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "528a2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# GPU acceleration for faster training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device being used: {device}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.01, device=device):\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        current_loss = 0.0\n",
    "        for batch_idx, (img, _) in enumerate(train_loader):\n",
    "            \n",
    "            img = img.to(device)\n",
    "            # tuple returned from foward(x)\n",
    "            reconstruction, latent = model(img)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # when the input equal to the target \n",
    "            loss = criterion(reconstruction, img)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item() * img.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"[Epoch {e+1}/{epochs}] \"\n",
    "                    f\"Step {batch_idx+1}/{len(train_loader)} \"\n",
    "                    f\"Batch loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_train_loss = current_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        current_loss_val = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img, _ in val_loader:\n",
    "                img = img.to(device)\n",
    "                reconstruction, latent = model(img)\n",
    "                loss = criterion(reconstruction, img)\n",
    "                current_loss_val += loss.item() * img.size(0)\n",
    "\n",
    "        epoch_val_loss = current_loss_val / len(val_loader.dataset)\n",
    "        validation_losses.append(epoch_val_loss)\n",
    "\n",
    "        \n",
    "        print(f\"Epoch {e+1}/{epochs} \"\n",
    "            f\"- Train: {epoch_train_loss:.4f}, Val: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "    return model, train_losses, validation_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Step 100/375 Batch loss: 0.0204\n",
      "[Epoch 1/10] Step 200/375 Batch loss: 0.0136\n",
      "[Epoch 1/10] Step 300/375 Batch loss: 0.0099\n",
      "Epoch 1/10 - Train: 0.0318, Val: 0.0088\n",
      "[Epoch 2/10] Step 100/375 Batch loss: 0.0078\n",
      "[Epoch 2/10] Step 200/375 Batch loss: 0.0071\n",
      "[Epoch 2/10] Step 300/375 Batch loss: 0.0068\n",
      "Epoch 2/10 - Train: 0.0077, Val: 0.0070\n",
      "[Epoch 3/10] Step 100/375 Batch loss: 0.0071\n",
      "[Epoch 3/10] Step 200/375 Batch loss: 0.0066\n",
      "[Epoch 3/10] Step 300/375 Batch loss: 0.0065\n",
      "Epoch 3/10 - Train: 0.0066, Val: 0.0066\n",
      "[Epoch 4/10] Step 100/375 Batch loss: 0.0060\n",
      "[Epoch 4/10] Step 200/375 Batch loss: 0.0059\n",
      "[Epoch 4/10] Step 300/375 Batch loss: 0.0062\n",
      "Epoch 4/10 - Train: 0.0061, Val: 0.0060\n",
      "[Epoch 5/10] Step 100/375 Batch loss: 0.0062\n",
      "[Epoch 5/10] Step 200/375 Batch loss: 0.0056\n",
      "[Epoch 5/10] Step 300/375 Batch loss: 0.0062\n",
      "Epoch 5/10 - Train: 0.0058, Val: 0.0057\n",
      "[Epoch 6/10] Step 100/375 Batch loss: 0.0056\n",
      "[Epoch 6/10] Step 200/375 Batch loss: 0.0057\n",
      "[Epoch 6/10] Step 300/375 Batch loss: 0.0056\n",
      "Epoch 6/10 - Train: 0.0055, Val: 0.0055\n",
      "[Epoch 7/10] Step 100/375 Batch loss: 0.0054\n",
      "[Epoch 7/10] Step 200/375 Batch loss: 0.0055\n",
      "[Epoch 7/10] Step 300/375 Batch loss: 0.0052\n",
      "Epoch 7/10 - Train: 0.0053, Val: 0.0053\n",
      "[Epoch 8/10] Step 100/375 Batch loss: 0.0055\n",
      "[Epoch 8/10] Step 200/375 Batch loss: 0.0057\n",
      "[Epoch 8/10] Step 300/375 Batch loss: 0.0057\n",
      "Epoch 8/10 - Train: 0.0052, Val: 0.0051\n",
      "[Epoch 9/10] Step 100/375 Batch loss: 0.0050\n",
      "[Epoch 9/10] Step 200/375 Batch loss: 0.0053\n",
      "[Epoch 9/10] Step 300/375 Batch loss: 0.0050\n",
      "Epoch 9/10 - Train: 0.0050, Val: 0.0050\n",
      "[Epoch 10/10] Step 100/375 Batch loss: 0.0048\n",
      "[Epoch 10/10] Step 200/375 Batch loss: 0.0045\n",
      "[Epoch 10/10] Step 300/375 Batch loss: 0.0048\n",
      "Epoch 10/10 - Train: 0.0050, Val: 0.0049\n"
     ]
    }
   ],
   "source": [
    "baseModel = ConvolutionAutoEncoder()\n",
    "trainedModel, train_losses, validation_losses = train_model(baseModel, train_loader, val_loader, epochs=1, learning_rate=0.001, device=device)\n",
    "\n",
    "# Saving the trained model\n",
    "torch.save(trainedModel.state_dict(), 'Base_conv_autoencoder.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fcd8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device=device):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fa4e8",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 4.1 Loss curves\n",
    "\n",
    "Figure 1 shows the evolution of the training and validation MSE loss over epochs.  \n",
    "The validation loss decreases and then stabilises after about 10 epochs, which indicates that the autoencoder has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ab57792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(train_losses, validation_losses):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(validation_losses, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Figure 1\")\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a02f",
   "metadata": {},
   "source": [
    "### 4.2 Test performance\n",
    "\n",
    "After training, we evaluate the model on the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a07295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device=device):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, _ in test_loader:\n",
    "            img = img.to(device)\n",
    "            reconstruction, latent = model(img) \n",
    "            # MSE between input and reconstruction\n",
    "            loss = criterion(reconstruction, img)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Test MSE: {test_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd6f98",
   "metadata": {},
   "source": [
    "### 4.3 Visual inspection\n",
    "\n",
    "Figure 2 illustrates some original input images and their reconstructions.  \n",
    "The reconstructions are slightly blurred but preserve the main objects and colours, which is expected for a simple convolutional autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f6c0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_reconstruction(model, test_loader, device=device, num_images=5, title=\"\"):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Take a single batch from the test (or val) loader\n",
    "    img, _ = next(iter(test_loader)) \n",
    "    img = img.to(device)\n",
    "\n",
    "    # 2. Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        reconstruction, latent = model(img)         \n",
    "\n",
    "    # 3. Move to CPU and (optionally) undo normalization\n",
    "    orig = img.cpu()\n",
    "    rec  = reconstruction.cpu()\n",
    "\n",
    "    # 4. Plot first 7 originals and their reconstructions\n",
    "    n_show = 7\n",
    "    fig, ax = plt.subplots(2, n_show, figsize=(15, 4), dpi=250)\n",
    "\n",
    "    for i in range(n_show):\n",
    "        # [C, H, W] -> [H, W, C]\n",
    "        img_np = orig[i].numpy().transpose((1, 2, 0))\n",
    "        rec_np = rec[i].numpy().transpose((1, 2, 0))\n",
    "\n",
    "        ax[0, i].imshow(img_np)\n",
    "        ax[0, i].axis('off')\n",
    "\n",
    "        ax[1, i].imshow(rec_np)\n",
    "        ax[1, i].axis('off')\n",
    "    ax[0, 0].set_title('Original')\n",
    "    ax[1, 0].set_title('Reconstructed')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # remember to add title to the graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Training model: Base\n",
      "======================\n",
      "[Epoch 1/5] Step 100/375 Batch loss: 0.0223\n",
      "[Epoch 1/5] Step 200/375 Batch loss: 0.0123\n",
      "[Epoch 1/5] Step 300/375 Batch loss: 0.0100\n",
      "Epoch 1/5 - Train: 0.0324, Val: 0.0098\n",
      "Validation/Test Loss for Base: 0.009768\n",
      "\n",
      "======================\n",
      "Training model: Simple\n",
      "======================\n",
      "[Epoch 1/5] Step 100/375 Batch loss: 0.0221\n",
      "[Epoch 1/5] Step 200/375 Batch loss: 0.0131\n",
      "[Epoch 1/5] Step 300/375 Batch loss: 0.0106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m======================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m trained_model, train_losses, validation_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Evaluate on test or validation set\u001b[39;00m\n\u001b[32m     21\u001b[39m val_loss = validate_model(trained_model, val_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, learning_rate, device)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# when the input equal to the target \u001b[39;00m\n\u001b[32m     28\u001b[39m loss = criterion(reconstruction, img)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m optimizer.step()\n\u001b[32m     33\u001b[39m current_loss += loss.item() * img.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luukj\\Documents\\UM\\Machine Learning\\Grayscale-colorizer\\my_env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "models_to_test = {\n",
    "    \"Base\": ConvolutionAutoEncoder(),\n",
    "    \"Simple\": ShallowConvolutionAutoEncoder(),\n",
    "    \"Deep\": DeepConvolutionAutoEncoder(),\n",
    "    \"WideKernel\": BiggerFilterConvolutionAutoEncoder(),\n",
    "    \"Strided\": StridedConvolutionAutoEncoder()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models_to_test.items():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"Training model: {name}\")\n",
    "    print(f\"======================\")\n",
    "\n",
    "    trained_model, train_losses, validation_losses = train_model(model, train_loader, val_loader, epochs=10, learning_rate=1e-3)\n",
    "    \n",
    "    # Saving the trained model\n",
    "    torch.save(trained_model.state_dict(), f'{name.lower()}_conv_autoencoder.pth')\n",
    "\n",
    "    # Evaluate on test or validation set\n",
    "    val_loss = validate_model(trained_model, val_loader)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"model\": trained_model,\n",
    "        \"val_loss\": val_loss\n",
    "    }\n",
    "\n",
    "    print(f\"Validation/Test Loss for {name}: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Show summary of results\n",
    "print(\"\\n Model Performance Summary:\")\n",
    "for name, entry in results.items():\n",
    "    print(f\"{name:<15} | Validation/Test Loss: {entry['val_loss']:.6f}\")\n",
    "\n",
    "# Optionally, show all the reconstructions\n",
    "for name, entry in results.items():\n",
    "    model = entry[\"model\"]\n",
    "    show_reconstruction(model, test_loader, device=device, title=f\"Reconstruction - {name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
